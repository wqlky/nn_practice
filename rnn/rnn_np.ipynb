{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "参考：  \n",
    "https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用numpy等基本库实现RNN\n",
    "\n",
    "# 公式\n",
    "## Notation\n",
    "输入序列：$x$   \n",
    "输出序列：$y$   \n",
    "输入，输出序列中的某个点: $ x^{<t>}, y^{<t>} $  \n",
    "输入，输出序列的长度: $ T_x^{(i)}, T_y^{(i)} $， 其中i表示第i个样本\n",
    "\n",
    "\n",
    "![rnn](rnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## forward\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{<t>} =& tanh( w_{aa}a^{<t-1>} + w_{ax}x^{<t>} + b_a )  \\\\\n",
    "\\\\\n",
    "\\hat y^{<t>} =& softmax(w_{ya}^{<t>} a^{<t>} + b_y) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## loss\n",
    "$$\n",
    "\\begin{align}\n",
    "E^{<t>} = & - \\sum_i y_i^{<t>} ln \\hat y_i^{<t>} \\\\\n",
    "E(y, \\hat y) =& - \\sum_t E^{<t>} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backpropagation\n",
    "\n",
    "### 对$w_{ya}, b_y$求导\n",
    "\n",
    "为了方便下面的推导，细化一下上面的forward公式。同时为了简化公式暂时省略$^{<t>}$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\\\\n",
    "z =& w_{ya} a + b_y \\\\\n",
    "z_i =& \\sum_j w_{ya\\_ij} a_j + b_{y\\_i}  \\\\\n",
    "\\\\\n",
    "\\hat y_i =& \\frac{ e^{z_i} }{ \\sum_j e^{z_j} }  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "损失函数：\n",
    "$$\n",
    "\\begin{align}\n",
    "E =& -\\sum_i y_i ln \\hat y_i  \\\\\n",
    "  =& -\\sum_i y_i ( z_i - ln \\sum_j e^{z_j}  ) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "求导\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E }{ \\partial w_{ya\\_ij} } =& \\sum_k \\frac{ \\partial E }{ \\partial z_k }\n",
    "                        \\frac{ \\partial z_k }{ \\partial w_{ya\\_ij}}   \\\\\n",
    "                                           =& \\frac{ \\partial E }{ \\partial z_i }\n",
    "                        \\frac{ \\partial z_i }{ \\partial w_{ya\\_ij}}                        \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E }{ \\partial z_i } =& \\frac{ \\partial ( -\\sum_{k \\neq i} y_k ( z_k - ln \\sum_j e^{z_j}  ) - y_i ( z_i - ln \\sum_j e^{z_j}  ) ) }{ \\partial z_i }  \\\\\n",
    "                                    =& -\\sum_{k \\neq i} y_k (0 - \\frac{e^{z_i}}{ \\sum_j e^{z_j} } ) - y_i (1 - \\frac{e^{z_i} }{ \\sum_j e^{z_j} } )  \\\\\n",
    "                                    =& \\sum_{k \\neq i} y_k \\frac{e^{z_i}}{ \\sum_j e^{z_j} } - y_i + \\frac{e^{z_i}}{ \\sum_j e^{z_j}}  \\\\\n",
    "                                    =& \\sum_k y_k \\frac{e^{z_i}}{ \\sum_j e^{z_j}} - y_i \\\\\n",
    "                                    =& \\frac{e^{z_i}}{ \\sum_j e^{z_j}} - y_i \\\\\n",
    "                                    =& \\hat y_i - y_i \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{ \\partial z_i }{ \\partial w_{ya\\_ij}} =& a_j\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "补上$^{<t>}$，得到\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial z_i^{<t>} } =&  \\hat y_i^{<t>} - y_i^{<t>} \\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial w_{ya\\_ij} } =& a_j^{<t>}( \\hat y_i^{<t>} - y_i^{<t>} )  \\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial b_{y\\_i} } =& \\hat y_i^{<t>} - y_i^{<t>} \n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对$w_{aa}, w_{ax}, b_a$求导\n",
    "细化前向公式\n",
    "$$\n",
    "\\begin{align}\n",
    "s_i^{<t>} =& \\sum_j w_{aa\\_ij}a_j^{<t-1>} + \\sum_j w_{ax\\_ij}x_j^{<t>} + b_{a\\_i} \\\\\n",
    "a_i^{<t>} =& tanh( s_i^{<t>} )  \\\\\n",
    "\\\\\n",
    "z_i^{<t>} =& \\sum_j w_{ya\\_ij} a_j^{<t>} + b_{y\\_i}  \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$w_{aa}$求导  \n",
    "在计算$\\frac{\\partial E^{<t>}}{\\partial w_{aa\\_ij}}$时需要考虑所有的$s_i^{<t'>}, 0<t' \\le t $\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial w_{aa\\_ij} } =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}}\n",
    "                                                              \\frac{\\partial s_i^{<t'>}}{\\partial w_{aa\\_ij}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中第二项$\\frac{\\partial s_i^{<t'>}}{\\partial w_{aa\\_ij}}=a_j^{<t'-1>}$。    \n",
    "第一项计算比较复杂，下面采用递归的方式计算。当$0<t'<t$时：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}} =& \\sum_k \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                                                       \\frac{\\partial s_k^{<t'+1>}}{\\partial s_i^{<t'>}} \\\\\n",
    "=& \\sum_k \\sum_m \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                 \\frac{\\partial s_k^{<t'+1>}}{\\partial a_m^{<t'>}}\n",
    "                 \\frac{\\partial a_m^{<t'>}}{\\partial s_i^{<t'>}}\\\\\n",
    "=& \\sum_k \\sum_m \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                w_{aa_km} \\delta_{mi} ( 1-(a_i^{<t'>})^2 ) \\\\\n",
    "=& \\sum_k \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                w_{aa_ki}( 1-(a_i^{<t'>})^2 ) \\\\\n",
    "=& \\sum_k w_{aa\\_ik}^T  \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}} ( 1-(a_i^{<t'>})^2 ) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递推公式的初始值:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{ \\partial s_i^{<t>}} =& \\sum_m \\sum_n \\frac{ \\partial E^{<t>} }{ \\partial z_m^{<t>} } \n",
    "                                                       \\frac{ \\partial z_m^{<t>}}{ \\partial a_n^{<t>} }\n",
    "                                                       \\frac{ \\partial a_n^{<t>}}{ \\partial s_i^{<t>} } \\\\\n",
    "=& \\sum_m \\sum_n (\\hat y_i^{<t>} - y_i^{<t>}) w_{ya\\_mn} \\delta_{ni} ( 1 - (a_i^{<t>})^2 )\\\\ \n",
    "=& \\sum_m (\\hat y_i^{<t>} - y_i^{<t>}) w_{ya\\_mi} ( 1 - (a_i^{<t>})^2 )\\\\ \n",
    "=& \\sum_m w_{ya\\_im}^T(\\hat y_i^{<t>} - y_i^{<t>})( 1 - (a_i^{<t>})^2 )\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在算好$\\frac{\\partial E^{<t>}}{\\partial s_i^{<t>}}$后，就比较容易得到对$w_{ax}, b_a$的导数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{\\partial w_{ax\\_ij}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}}\n",
    "                                                              \\frac{\\partial s_i^{<t'>}}{\\partial w_{ax\\_ij}} \\\\\n",
    "=& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}} x_j^{<t'>}     \\\\                                       \n",
    "\\\\\n",
    "\\frac{\\partial E^{<t>}}{\\partial b_{a\\_i}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t>}}\n",
    "                                                        \\frac{\\partial s_i^{<t>}}{\\partial b_{a\\_i}} \\\\\n",
    "=& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}}    \\\\    \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵形式   \n",
    "\n",
    "对$w_{ya}, b_y$的导数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial w_{ya} } =& (\\hat y^{<t>} - y^{<t>} ) \\otimes a^{<t>}   \\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial b_y}  =& \\hat y^{<t>} - y^{<t>} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E^{<t>}}{\\partial s^{<t>}}$的递推公式：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{ \\partial s^{<t>}} =& w_{ya}^T(\\hat y^{<t>} - y^{<t>}) * ( 1 - (a^{<t>})^2 )\\\\\n",
    "\\\\\n",
    "\\frac{\\partial E^{<t>}}{\\partial s^{<t'>}} =& w_{aa}^T \\frac{\\partial E^{<t>}}{\\partial s^{<t'+1>}} * ( 1-(a^{<t'>})^2 ) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$w_{aa}, w_{ax}, b_a$的导数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{\\partial w_{aa}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s^{<t'>}} \\otimes a^{<t'-1>}     \\\\                   \n",
    "\\frac{\\partial E^{<t>}}{\\partial w_{ax}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s^{<t'>}} \\otimes x^{<t'>}\n",
    "\\\\\n",
    "\\frac{\\partial E^{<t>}}{\\partial b_a} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s^{<t'>}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import logging\n",
    "from datetime import datetime \n",
    "import pickle\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp( x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_NP(object):\n",
    "    def __init__(self, hid_dim, inp_dim, out_dim):\n",
    "        self.w_ax = np.random.uniform(-np.sqrt(1.0/inp_dim), np.sqrt(1.0/inp_dim), (hid_dim, inp_dim))\n",
    "        self.w_aa = np.random.uniform(-np.sqrt(1.0/hid_dim), np.sqrt(1.0/hid_dim), (hid_dim, hid_dim))\n",
    "        self.w_ya = np.random.uniform(-np.sqrt(1.0/hid_dim), np.sqrt(1.0/hid_dim), (out_dim, hid_dim))\n",
    "        self.b_a = np.zeros( (hid_dim,), dtype=float )\n",
    "        self.b_y = np.zeros( (out_dim,), dtype=float )\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.index_to_word = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        T = len(x)\n",
    "        a = np.zeros( (T + 1, self.hid_dim), dtype=float )\n",
    "      \n",
    "        w_ax = self.w_ax\n",
    "        w_aa = self.w_aa\n",
    "        w_ya = self.w_ya\n",
    "        b_a = self.b_a\n",
    "        b_y = self.b_y\n",
    "        y_hat = np.zeros( (T, self.out_dim), dtype=float )\n",
    "        for t in range(T):\n",
    "            #a[t] = np.tanh(np.dot(w_aa, a[t-1]) + np.dot(w_ax, x[t]) + b_a) \n",
    "            a[t] = np.tanh(np.dot(w_aa, a[t-1]) + w_ax[:, x[t]] + b_a) #assume x is onehot \n",
    "            z = np.dot(w_ya, a[t]) + b_y\n",
    "            y_hat[t] = softmax(z)\n",
    "            \n",
    "        return [y_hat, a]\n",
    "            \n",
    "    def sample(self, stop_word_index, max_len):\n",
    "      \n",
    "        seq = []\n",
    "        last_word_index = None\n",
    "        a = np.zeros( (self.hid_dim,), dtype=float )\n",
    "        x = np.zeros( (self.inp_dim,), dtype=float)\n",
    "        \n",
    "        w_ax = self.w_ax\n",
    "        w_aa = self.w_aa\n",
    "        w_ya = self.w_ya\n",
    "        b_a = self.b_a\n",
    "        b_y = self.b_y\n",
    "        while last_word_index != stop_word_index and len(seq) < max_len:\n",
    "            #a = np.tanh(np.dot(w_aa, a) + w_ax[:, last_word_index] + b_a) \n",
    "            a = np.tanh(np.dot(w_aa, a) + np.dot(w_ax, x) + b_a) \n",
    "            z = np.dot(w_ya, a) + b_y\n",
    "            y_hat = softmax(z)\n",
    "    \n",
    "            last_word_index = np.random.choice(range(self.out_dim), p=y_hat)\n",
    "            seq.append(last_word_index)\n",
    "            x = y_hat\n",
    "        return seq\n",
    "            \n",
    "        \n",
    "    def predict(self, x):\n",
    "        y_hat, _ = self.forward(x)\n",
    "        return np.argmax(y_hat, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, xs, ylabels):\n",
    "        L = 0\n",
    "        for i in np.arange(len(ylabels)):\n",
    "            y_hat, a = self.forward(xs[i])\n",
    "            correct_word_predictions = y_hat[np.arange(len(ylabels[i])), ylabels[i]]\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "\n",
    "    def calculate_loss(self, xs, ylables):\n",
    "        N = sum([len(y) for y in ylables])\n",
    "        return self.calculate_total_loss(xs,ylables)/N\n",
    "    \n",
    "    def backpropagation(self, x, ylabel):\n",
    "        T = len(ylabel)\n",
    "        y_hat, a = self.forward(x)\n",
    "        dw_ax = np.zeros_like(self.w_ax)\n",
    "        dw_aa = np.zeros_like(self.w_aa)\n",
    "        dw_ya = np.zeros_like(self.w_ya)\n",
    "        db_a = np.zeros_like(self.b_a)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        delta_y = y_hat\n",
    "        delta_y[np.arange(T), ylabel] -= 1.0\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dw_ya += np.outer( delta_y[t], a[t] )\n",
    "            db_y += delta_y[t]\n",
    "            \n",
    "            delta_t = self.w_ya.T.dot( delta_y[t] ) * ( 1 - (a[t] ** 2) )\n",
    "\n",
    "            for _t in np.arange(t+1)[::-1]:\n",
    "                dw_aa += np.outer(delta_t, a[_t-1])\n",
    "                #dw_ax += np.outer(delta_t, x[_t])\n",
    "                dw_ax[:, x[_t]] += delta_t #assume x is onehot\n",
    "                db_a += delta_t\n",
    "                delta_t = self.w_aa.T.dot(delta_t) * ( 1 - a[_t-1]**2 )\n",
    "                \n",
    "        return [dw_aa, dw_ax, db_a, dw_ya, db_y]\n",
    "    \n",
    "    def train(self, xs, ylabels, learning_rate=0.005, nepoch=100, evaluate_epoch_num = 1, save_model_fmt = None):\n",
    "        \n",
    "        all_sample_num = len(ylabels)\n",
    "        evaluate_num = min(1000, all_sample_num)\n",
    "        evaluate_xs = xs[:evaluate_num]\n",
    "        evaluate_ylabels = ylabels[:evaluate_num]\n",
    "        for epoch in range(nepoch):\n",
    "                \n",
    "            for i in range(all_sample_num):\n",
    "                if i % 1000 == 0:\n",
    "                    logging.info(\"samples %d/%d\", i, all_sample_num)\n",
    "                dw_aa, dw_ax, db_a, dw_ya, db_y = self.backpropagation(xs[i], ylabels[i])\n",
    "                self.w_aa -= learning_rate * dw_aa\n",
    "                self.w_ax -= learning_rate * dw_ax\n",
    "                self.b_a -= learning_rate * db_a\n",
    "                self.w_ya -= learning_rate * dw_ya\n",
    "                self.b_y -= learning_rate * db_y\n",
    "                \n",
    "            if (epoch % evaluate_epoch_num == 0) or epoch >= (nepoch-1):\n",
    "                loss = self.calculate_loss(evaluate_xs, evaluate_ylabels)\n",
    "                logging.info(\"time:%s epoch:%d loss:%f\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch, loss)\n",
    "                if save_model_fmt is not None:\n",
    "                    model_path = save_model_fmt % epoch\n",
    "                    with open(model_path, \"wb\") as f:\n",
    "                        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "                        logging.info(\"save model to %s\", model_path)\n",
    "                \n",
    "        logging.info(\"train done\")\n",
    "       \n",
    "                \n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        bptt_gradients = self.backpropagation(x, y)\n",
    "   \n",
    "        param_names = ['w_aa', 'w_ax', 'b_a', 'w_ya', 'b_y']\n",
    "        for pidx, pname in enumerate(param_names):\n",
    "            param = operator.attrgetter(pname)(self)\n",
    "            logging.info(\"Performing gradient check for parameter %s with shape:%s\", pname, param.shape)\n",
    "            \n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                org_value = param[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                param[ix] = org_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                param[ix] = org_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                param[ix] = org_value\n",
    "                \n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "         \n",
    "                if np.abs(backprop_gradient - estimated_gradient) > error_threshold * (\n",
    "                    np.abs(backprop_gradient) + np.abs(estimated_gradient) ):\n",
    "                    logging.info(\"Gradient Check ERROR: parameter=%s ix=%s\", pname, ix)\n",
    "                    logging.info(\"+h Loss: %f\", gradplus)\n",
    "                    logging.info(\"-h Loss: %f\", gradminus)\n",
    "                    logging.info(\"Estimated_gradient: %f\", estimated_gradient)\n",
    "                    logging.info(\"Backpropagation gradient: %f\", backprop_gradient)\n",
    "                    logging.info(\"Relative Error: %f\", relative_error)\n",
    "                     \n",
    "                it.iternext()\n",
    "            logging.info(\"Gradient check for parameter %s passed.\", pname)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_onehot(x,n):\n",
    "    e = np.zeros( (n,), dtype=float )\n",
    "    e[x] = 1\n",
    "    return e\n",
    "    \n",
    "def test_gradient_check():\n",
    "    np.random.seed(0)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    model = RNN_NP(10, 5, 5)\n",
    "    x = [0,1,2,3]\n",
    "    #x = [ encoder_onehot(i, 5) for i in x ]\n",
    "    #import pdb; pdb.set_trace()\n",
    "    model.gradient_check(x, [1,2,3,4])\n",
    "#test_gradient_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH=\"model\"\n",
    "def load_model():\n",
    "    exist_model_path = \"%s/rnn_np.model\"%MODEL_PATH\n",
    "    if os.path.exists(exist_model_path):\n",
    "        with open(exist_model_path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "            logging.info(\"load model from %s\", exist_model_path)\n",
    "            return model\n",
    "    return None\n",
    "            \n",
    "def save_model(model):\n",
    "    model_path = \"%s/rnn_np.model\"%MODEL_PATH\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)\n",
    "        logging.info(\"save model to %s\", model_path)\n",
    "\n",
    "def test_train():\n",
    "\n",
    "    vocab_num = 5000\n",
    "    from data import TextDataLoader\n",
    "    loader = TextDataLoader()\n",
    "    loader.load_data(\"data/poetry.txt\", vocab_num)\n",
    "\n",
    "    model = load_model()\n",
    "    if model is None:\n",
    "        model = RNN_NP(32, vocab_num, vocab_num)\n",
    "    \n",
    "    model.index_to_word = loader.index_to_word\n",
    "    \n",
    "    model.train(loader.data, loader.data, learning_rate=0.005, \n",
    "                nepoch=32, evaluate_epoch_num = 1, \n",
    "                save_model_fmt = MODEL_PATH+\"/rnn_np_%d.model\")\n",
    "    \n",
    "    save_model(model)\n",
    "        \n",
    "    \n",
    "def test_gen():\n",
    "    model = load_model()\n",
    "    \n",
    "    '''\n",
    "    from data import TextDataLoader\n",
    "    loader = TextDataLoader()\n",
    "    loader.load_data(\"data/poetry.txt\", 5000)\n",
    "    loss = model.calculate_loss(loader.data[:100], loader.data[:100])\n",
    "    logging.info(\"model loss:%f\", loss)\n",
    "    '''\n",
    "\n",
    "    stop_word_index = 1\n",
    "    seq = model.sample(stop_word_index, 10)\n",
    "    print( [ model.index_to_word[i] for i in seq ]  )\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-01-31 19:22:58][<ipython-input-415-f98279a29242>:7]load model from model/rnn_np.model\n",
      "['仇', '裛', '暾', '鳌', '缥', '侪', '炎', '玙', '髻', '辩']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig( \n",
    "                    level=logging.DEBUG,\n",
    "                    format='[%(asctime)s][%(filename)s:%(lineno)d]%(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    handlers=[\n",
    "                            logging.StreamHandler(sys.stdout),\n",
    "                            logging.FileHandler(filename='%s/rnn.log'%MODEL_PATH)\n",
    "                        ])\n",
    "    \n",
    "\n",
    "    #test_train()\n",
    "    test_gen()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
