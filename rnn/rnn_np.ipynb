{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://github.com/krocki/dnc/blob/master/rnn-numpy.py    \n",
    "https://blog.csdn.net/tudaodiaozhale/article/details/80464060\n",
    "    \n",
    "    \n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/     \n",
    "https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用numpy等基本库实现RNN\n",
    "\n",
    "# 公式\n",
    "## Notation\n",
    "输入序列：$x$   \n",
    "输出序列：$y$   \n",
    "输入，输出序列中的某个点: $ x^{<t>}, y^{<t>} $  \n",
    "输入，输出序列的长度: $ T_x^{(i)}, T_y^{(i)} $， 其中i表示第i个样本\n",
    "\n",
    "\n",
    "![rnn](rnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## forward\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{<t>} =& tanh( w_{aa}a^{<t-1>} + w_{ax}x^{<t>} + b_a )  \\\\\n",
    "\\\\\n",
    "\\hat y^{<t>} =& softmax(w_{ya}^{<t>} a^{<t>} + b_y) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## loss\n",
    "$$\n",
    "\\begin{align}\n",
    "E^{<t>} = & - \\sum_i y_i^{<t>} ln \\hat y_i^{<t>} \\\\\n",
    "E(y, \\hat y) =& - \\sum_t E^{<t>} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backpropagation\n",
    "\n",
    "### 对$w_{ya}, b_y$求导\n",
    "\n",
    "为了方便下面的推导，细化一下上面的forward公式。同时为了简化公式暂时省略$^{<t>}$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\\\\n",
    "z =& w_{ya} a + b_y \\\\\n",
    "z_i =& \\sum_j w_{ya\\_ij} a_j + b_{y\\_i}  \\\\\n",
    "\\\\\n",
    "\\hat y_i =& \\frac{ e^{z_i} }{ \\sum_j e^{z_j} }  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "损失函数：\n",
    "$$\n",
    "\\begin{align}\n",
    "E =& -\\sum_i y_i ln \\hat y_i  \\\\\n",
    "  =& -\\sum_i y_i ( z_i - ln \\sum_j e^{z_j}  ) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "求导\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E }{ \\partial w_{ya\\_ij} } =& \\sum_k \\frac{ \\partial E }{ \\partial z_k }\n",
    "                        \\frac{ \\partial z_k }{ \\partial w_{ya\\_ij}}   \\\\\n",
    "                                           =& \\frac{ \\partial E }{ \\partial z_i }\n",
    "                        \\frac{ \\partial z_i }{ \\partial w_{ya\\_ij}}                        \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E }{ \\partial z_i } =& \\frac{ \\partial ( -\\sum_{k \\neq i} y_k ( z_k - ln \\sum_j e^{z_j}  ) - y_i ( z_i - ln \\sum_j e^{z_j}  ) ) }{ \\partial z_i }  \\\\\n",
    "                                    =& -\\sum_{k \\neq i} y_k (0 - \\frac{e^{z_i}}{ \\sum_j e^{z_j} } ) - y_i (1 - \\frac{e^{z_i} }{ \\sum_j e^{z_j} } )  \\\\\n",
    "                                    =& \\sum_{k \\neq i} y_k \\frac{e^{z_i}}{ \\sum_j e^{z_j} } - y_i + \\frac{e^{z_i}}{ \\sum_j e^{z_j}}  \\\\\n",
    "                                    =& \\sum_k y_k \\frac{e^{z_i}}{ \\sum_j e^{z_j}} - y_i \\\\\n",
    "                                    =& \\frac{e^{z_i}}{ \\sum_j e^{z_j}} - y_i \\\\\n",
    "                                    =& \\hat y_i - y_i \\\\\n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{ \\partial z_i }{ \\partial w_{ya\\_ij}} =& a_j\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "补上$^{<t>}$，得到\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial z_i^{<t>} } =&  \\hat y_i^{<t>} - y_i^{<t>} \\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial w_{ya\\_ij} } =& a_j^{<t>}( \\hat y_i^{<t>} - y_i^{<t>} )  \\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial b_{y\\_i} } =& \\hat y_i^{<t>} - y_i^{<t>} \n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对$w_{aa}, w_{ax}, b_a$求导\n",
    "细化前向公式\n",
    "$$\n",
    "\\begin{align}\n",
    "s_i^{<t>} =& \\sum_j w_{aa\\_ij}a_j^{<t-1>} + \\sum_j w_{ax\\_ij}x_j^{<t>} + b_{a\\_i} \\\\\n",
    "a_i^{<t>} =& tanh( s_i^{<t>} )  \\\\\n",
    "\\\\\n",
    "z_i^{<t>} =& \\sum_j w_{ya\\_ij} a_j^{<t>} + b_{y\\_i}  \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$w_{aa}$求导  \n",
    "在计算$\\frac{\\partial E^{<t>}}{\\partial w_{aa\\_ij}}$时需要考虑所有的$s_i^{<t'>}, 0<t' \\le t $\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial w_{aa\\_ij} } =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}}\n",
    "                                                              \\frac{\\partial s_i^{<t'>}}{\\partial w_{aa\\_ij}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中第二项$\\frac{\\partial s_i^{<t'>}}{\\partial w_{aa\\_ij}}=a_j^{<t'-1>}$。    \n",
    "第一项计算比较复杂，下面采用递归的方式计算。当$0<t'<t$时：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}} =& \\sum_k \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                                                       \\frac{\\partial s_k^{<t'+1>}}{\\partial s_i^{<t'>}} \\\\\n",
    "=& \\sum_k \\sum_m \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                 \\frac{\\partial s_k^{<t'+1>}}{\\partial a_m^{<t'>}}\n",
    "                 \\frac{\\partial a_m^{<t'>}}{\\partial s_i^{<t'>}}\\\\\n",
    "=& \\sum_k \\sum_m \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                w_{aa_km} \\delta_{mi} ( 1-(a_i^{<t'>})^2 ) \\\\\n",
    "=& \\sum_k \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}}\n",
    "                w_{aa_ki}( 1-(a_i^{<t'>})^2 ) \\\\\n",
    "=& \\sum_k w_{aa\\_ik}^T  \\frac{\\partial E^{<t>}}{\\partial s_k^{<t'+1>}} ( 1-(a_i^{<t'>})^2 ) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递推公式的初始值:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{ \\partial s_i^{<t>}} =& \\sum_m \\sum_n \\frac{ \\partial E^{<t>} }{ \\partial z_m^{<t>} } \n",
    "                                                       \\frac{ \\partial z_m^{<t>}}{ \\partial a_n^{<t>} }\n",
    "                                                       \\frac{ \\partial a_n^{<t>}}{ \\partial s_i^{<t>} } \\\\\n",
    "=& \\sum_m \\sum_n (\\hat y_i^{<t>} - y_i^{<t>}) w_{ya\\_mn} \\delta_{ni} ( 1 - (a_i^{<t>})^2 )\\\\ \n",
    "=& \\sum_m (\\hat y_i^{<t>} - y_i^{<t>}) w_{ya\\_mi} ( 1 - (a_i^{<t>})^2 )\\\\ \n",
    "=& \\sum_m w_{ya\\_im}^T(\\hat y_i^{<t>} - y_i^{<t>})( 1 - (a_i^{<t>})^2 )\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在算好$\\frac{\\partial E^{<t>}}{\\partial s_i^{<t>}}$后，就比较容易得到对$w_{ax}, b_a$的导数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{\\partial w_{ax\\_ij}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}}\n",
    "                                                              \\frac{\\partial s_i^{<t'>}}{\\partial w_{ax\\_ij}} \\\\\n",
    "=& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}} x_j^{<t'>}     \\\\                                       \n",
    "\\\\\n",
    "\\frac{\\partial E^{<t>}}{\\partial b_{a\\_i}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t>}}\n",
    "                                                        \\frac{\\partial s_i^{<t>}}{\\partial b_{a\\_i}} \\\\\n",
    "=& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s_i^{<t'>}}    \\\\    \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵形式   \n",
    "\n",
    "对$w_{ya}, b_y$的导数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial w_{ya} } =& (\\hat y^{<t>} - y^{<t>} ) \\otimes a^{<t>}   \\\\\n",
    "\\\\\n",
    "\\frac{ \\partial E^{<t>} }{ \\partial b_y}  =& \\hat y^{<t>} - y^{<t>} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial E^{<t>}}{\\partial s^{<t>}}$的递推公式：\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{ \\partial s^{<t>}} =& w_{ya}^T(\\hat y^{<t>} - y^{<t>}) * ( 1 - (a^{<t>})^2 )\\\\\n",
    "\\\\\n",
    "\\frac{\\partial E^{<t>}}{\\partial s^{<t'>}} =& w_{aa}^T \\frac{\\partial E^{<t>}}{\\partial s^{<t'+1>}} * ( 1-(a^{<t'>})^2 ) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对$w_{aa}, w_{ax}, b_a$的导数\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E^{<t>}}{\\partial w_{aa}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s^{<t'>}} \\otimes a^{<t'-1>}     \\\\                   \n",
    "\\frac{\\partial E^{<t>}}{\\partial w_{ax}} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s^{<t'>}} \\otimes x^{<t'>}\n",
    "\\\\\n",
    "\\frac{\\partial E^{<t>}}{\\partial b_a} =& \\sum_{t'} \\frac{\\partial E^{<t>}}{\\partial s^{<t'>}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp( x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_NP(object):\n",
    "    def __init__(self, hid_dim, inp_dim, out_dim):\n",
    "        self.w_ax = np.random.uniform(-np.sqrt(1.0/inp_dim), np.sqrt(1.0/inp_dim), (hid_dim, inp_dim))\n",
    "        self.w_aa = np.random.uniform(-np.sqrt(1.0/hid_dim), np.sqrt(1.0/hid_dim), (hid_dim, hid_dim))\n",
    "        self.w_ya = np.random.uniform(-np.sqrt(1.0/hid_dim), np.sqrt(1.0/hid_dim), (out_dim, hid_dim))\n",
    "        self.b_a = np.zeros( (hid_dim,), dtype=float )\n",
    "        self.b_y = np.zeros( (out_dim,), dtype=float )\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        T = len(x)\n",
    "        a = np.zeros( (T + 1, self.hid_dim), dtype=float )\n",
    "      \n",
    "        w_ax = self.w_ax\n",
    "        w_aa = self.w_aa\n",
    "        w_ya = self.w_ya\n",
    "        b_a = self.b_a\n",
    "        b_y = self.b_y\n",
    "        y_hat = np.zeros( (T, self.out_dim), dtype=float )\n",
    "        for t in range(T):\n",
    "            a[t] = np.tanh(np.dot(w_aa, a[t-1]) + np.dot(w_ax, x[t]) + b_a)\n",
    "            z = np.dot(w_ya, a[t]) + b_y\n",
    "            y_hat[t] = softmax(z)\n",
    "            \n",
    "        return [y_hat, a]\n",
    "            \n",
    "    def predict(self, x):\n",
    "        y_hat, _ = self.forward(x)\n",
    "        return np.argmax(y_hat, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, xs, ylabels):\n",
    "        L = 0\n",
    "        for i in np.arange(len(ylabels)):\n",
    "            y_hat, a = self.forward(xs[i])\n",
    "            correct_word_predictions = y_hat[np.arange(len(ylabels[i])), ylabels[i]]\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    "\n",
    "    def calculate_loss(self, xs, ylables):\n",
    "        N = np.sum((len(y) for y in ylables))\n",
    "        return self.calculate_total_loss(xs,ylables)/N\n",
    "    \n",
    "    def backpropagation(self, x, ylabel):\n",
    "        T = len(ylabel)\n",
    "        y_hat, a = self.forward(x)\n",
    "        dw_ax = np.zeros_like(self.w_ax)\n",
    "        dw_aa = np.zeros_like(self.w_aa)\n",
    "        dw_ya = np.zeros_like(self.w_ya)\n",
    "        db_a = np.zeros_like(self.b_a)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        delta_y = y_hat\n",
    "        delta_y[np.arange(T), ylabel] -= 1.0\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dw_ya += np.outer( delta_y[t], a[t] )\n",
    "            db_y += delta_y[t]\n",
    "            \n",
    "            delta_t = self.w_ya.T.dot( delta_y[t] ) * ( 1 - (a[t] ** 2) )\n",
    "\n",
    "            for _t in np.arange(t+1)[::-1]:\n",
    "                dw_aa += np.outer(delta_t, a[_t-1])\n",
    "                dw_ax += np.outer(delta_t, x[_t])\n",
    "                db_a += delta_t\n",
    "                delta_t = self.w_aa.T.dot(delta_t) * ( 1 - a[_t-1]**2 )\n",
    "                \n",
    "        return [dw_aa, dw_ax, db_a, dw_ya, db_y]\n",
    "    \n",
    "    \n",
    "    def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "        bptt_gradients = self.backpropagation(x, y)\n",
    "   \n",
    "        param_names = ['w_aa', 'w_ax', 'b_a', 'w_ya', 'b_y']\n",
    "        for pidx, pname in enumerate(param_names):\n",
    "            param = operator.attrgetter(pname)(self)\n",
    "            logging.info(\"Performing gradient check for parameter %s with shape:%s\", pname, param.shape)\n",
    "            \n",
    "            it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "            while not it.finished:\n",
    "                ix = it.multi_index\n",
    "                org_value = param[ix]\n",
    "                # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "                param[ix] = org_value + h\n",
    "                gradplus = self.calculate_total_loss([x],[y])\n",
    "                param[ix] = org_value - h\n",
    "                gradminus = self.calculate_total_loss([x],[y])\n",
    "                estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "                param[ix] = org_value\n",
    "                \n",
    "                # The gradient for this parameter calculated using backpropagation\n",
    "                backprop_gradient = bptt_gradients[pidx][ix]\n",
    "         \n",
    "                if np.abs(backprop_gradient - estimated_gradient) > error_threshold * (\n",
    "                    np.abs(backprop_gradient) + np.abs(estimated_gradient) ):\n",
    "                    logging.info(\"Gradient Check ERROR: parameter=%s ix=%s\", pname, ix)\n",
    "                    logging.info(\"+h Loss: %f\", gradplus)\n",
    "                    logging.info(\"-h Loss: %f\", gradminus)\n",
    "                    logging.info(\"Estimated_gradient: %f\", estimated_gradient)\n",
    "                    logging.info(\"Backpropagation gradient: %f\", backprop_gradient)\n",
    "                    logging.info(\"Relative Error: %f\", relative_error)\n",
    "                     \n",
    "                it.iternext()\n",
    "            logging.info(\"Gradient check for parameter %s passed.\", pname)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter w_aa with shape:(10, 10)\n",
      "Gradient check for parameter w_aa passed.\n",
      "Performing gradient check for parameter w_ax with shape:(10, 5)\n",
      "Gradient check for parameter w_ax passed.\n",
      "Performing gradient check for parameter b_a with shape:(10,)\n",
      "Gradient check for parameter b_a passed.\n",
      "Performing gradient check for parameter w_ya with shape:(5, 10)\n",
      "Gradient check for parameter w_ya passed.\n",
      "Performing gradient check for parameter b_y with shape:(5,)\n",
      "Gradient check for parameter b_y passed.\n"
     ]
    }
   ],
   "source": [
    "def encoder_onehot(x,n):\n",
    "    e = np.zeros( (n,), dtype=float )\n",
    "    e[x] = 1\n",
    "    return e\n",
    "\n",
    "def print_log(fmt, *params):\n",
    "    print(fmt % (params))\n",
    "logging.info = print_log\n",
    "    \n",
    "def test_gradient_check():\n",
    "    np.random.seed(0)\n",
    "    model = RNN_NP(10, 5, 5)\n",
    "    x = [ encoder_onehot(i, 5) for i in [0,1,2,3] ]\n",
    "    #import pdb; pdb.set_trace()\n",
    "    model.gradient_check(x, [1,2,3,4])\n",
    "test_gradient_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
